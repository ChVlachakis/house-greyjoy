# house-greyjoy
Project week5 - week6

## Design Effectiveness

### Was the experiment well-structured?
Yes. The dataset clearly separates sessions into a Control group and a Test group using the Variation column. The same metrics and definitions, such as conversion, completion time, and error indicators, were applied to both groups. This ensures that the comparison focuses only on the impact of the redesign.

### Were clients randomly and equally divided?
Yes. The number of sessions in the Control and Test groups was checked and found to be similar, with no large imbalance between the two. This shows that users were distributed evenly and that both designs were exposed to a comparable number of users.

### Were there any biases?
Potential bias was assessed by comparing baseline characteristics, including age, tenure, and prior engagement, between the two groups. These checks did not show meaningful differences. This suggests that the two groups were comparable before the experiment and that the results are not driven by demographic or behavioral bias.
Overall, the experiment design provides a reliable foundation for evaluating the impact of the new design.


## Duration Assessment

### Was the timeframe of the experiment (from 3/15/2017 to 6/20/2017) adequate to gather meaningful data and insights?
The timeframe was adequate to gather meaningful data and insights.
Running the experiment from March 15 to June 20, 2017 (about 14 weeks) gave enough time to observe stable trends, capture repeated weekly patterns, and reduce the impact of short-term fluctuations.
It also allowed key metrics (such as visits, conversions, or engagement) to accumulate, making comparisons between groups more reliable for short-term conclusions.
However, this period does not capture longer-term seasonality (e.g., quarterly or yearly effects), so results may not fully generalize beyond this time window.

## Additional Data Needs

### What other data, if available, could enhance the analysis?
Tracking users across multiple sessions would make it possible to understand learning effects over time. Customer surveys or satisfaction scores could add valuable context to the observed behavior. Education-related data could help assess whether users with different levels of financial or educational background experience the process differently. Information about device type and browser or app version would help identify whether the redesign performs differently across platforms.
